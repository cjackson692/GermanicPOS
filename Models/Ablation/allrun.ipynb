{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbformat\n",
    "from nbconvert.preprocessors import ExecutePreprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'AblationStudyOE.ipynb'\n",
    "with open(filename) as ff:\n",
    "    nb_in = nbformat.read(ff, nbformat.NO_CONVERT)\n",
    "    \n",
    "ep = ExecutePreprocessor(kernel_name='python3')\n",
    "\n",
    "nb_out = ep.preprocess(nb_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbformat\n",
    "from nbconvert.preprocessors import ExecutePreprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "CellExecutionError",
     "evalue": "An error occurred while executing the following cell:\n------------------\niteration = 0\nwhile iteration < 100:\n  print(iteration)\n  for size in sample_sizes:\n    random_indices = random.sample(range(len(base_sents)), size)\n    sents = []\n    tags = []\n    for index in random_indices:\n      sents.append(base_sents[index])\n      tags.append(base_tags[index])\n    for idx, sent in enumerate(sents):\n      sent.insert(0, 'start')\n      sent.append('stop')\n      sents[idx] = sent\n    for idx, sent in enumerate(tags):\n      sent.insert(0, 'start')\n      sent.append('stop')\n      tags[idx] = sent\n    rawwords = []\n    for sent in sents:\n      for word in sent:\n        rawwords.append(word)\n\n    rawtags = []\n    for sequence in tags:\n      for tag in sequence:\n        rawtags.append(tag)\n\n    allwords = list(set(rawwords))\n    alltags = list(set(rawtags))\n\n    word_tokenizer = {word: idx+1 for idx, word in enumerate(allwords)}\n    word_decoder = {idx+1: word for idx, word in enumerate(allwords)}\n    tag_tokenizer = {tag: idx+1 for idx, tag in enumerate(alltags)}\n    tag_decoder = {idx+1: tag for idx, tag in enumerate(alltags)}\n\n    def tokenize(sentences, tokenizer):\n      indexed_sentences = []\n      for sentence in sentences:\n        indexed_sentence = [tokenizer[word] for word in sentence]\n        indexed_sentences.append(indexed_sentence)\n      return indexed_sentences\n\n    encsents = tokenize(sents, word_tokenizer)\n    enctags = tokenize(tags, tag_tokenizer)\n\n    padsents, padtags = [], []\n\n    maxlen = max(len(sublist) for sublist in encsents)\n    for sublist in encsents:\n      while len(sublist) < maxlen:\n        sublist = sublist + [0]\n        if len(sublist) == maxlen:\n          break\n        sublist = [0] + sublist\n      padsents.append(sublist)\n    allenctags = []\n    maxlen = max(len(sublist) for sublist in enctags)\n    for sublist in enctags:\n      for i in sublist:\n        allenctags.append(i)\n      while len(sublist) < maxlen:\n        sublist = sublist + [0]\n        if len(sublist) == maxlen:\n          break\n        sublist = [0] + sublist\n      padtags.append(sublist)\n\n    tag_mask = []\n    for seq in padtags:\n      mask = [1]*len(seq)\n      for idx, tag in enumerate(seq):\n        if tag == tag_tokenizer['XX']:\n          mask[idx] = 0\n        if tag == tag_tokenizer['start']:\n          mask[idx] = 0\n        if tag == tag_tokenizer['stop']:\n          mask[idx] = 0\n        if tag == tag_tokenizer['PUNCT']:\n          mask[idx] = 0\n      tag_mask.append(mask)\n    for i in range(len(tag_mask)):\n      if len(tag_mask[i]) != len(padsents[i]):\n        print(i)\n    pad_tags_mask = []\n    for i in range(len(padtags)):\n      pad_tags_mask.append([padtags[i], tag_mask[i]])\n    X_test, X_train, y_test, y_train = train_test_split(padsents, pad_tags_mask, test_size=0.8)\n    X_train_tensor = torch.tensor(X_train, dtype=torch.long).to(device)\n    y_train_tensor = torch.tensor(y_train, dtype=torch.long).to(device)\n    X_test_tensor = torch.tensor(X_test, dtype=torch.long).to(device)\n    y_test_tensor = torch.tensor(y_test, dtype=torch.long).to(device)\n    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n    batch_size = 64\n    train_loader = DataLoader(train_dataset, batch_size=batch_size)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n\n    class_weights = calculate_class_weights(allenctags)\n\n    class Model(nn.Module):\n      def __init__(self):\n        super().__init__()\n\n      def categorical_accuracy(self, preds, y, mask, tag_pad_idx = 0):\n        max_preds = preds.argmax(dim = 1, keepdim = False)\n        max_preds = max_preds*mask\n        y = y*mask\n        max_preds = torch.flatten(max_preds)\n        y = torch.flatten(y)\n        non_pad_elements = y.nonzero()\n        correct = max_preds[non_pad_elements].eq(y[non_pad_elements])\n        return correct.sum() / y[non_pad_elements].shape[0]\n\n      def early_stop(self, validation_loss, patience = 3, min_delta = 0):\n        if validation_loss < self.min_validation_loss:\n            self.min_validation_loss = validation_loss\n            self.patiencecount = 0\n        elif validation_loss > (self.min_validation_loss + min_delta):\n            self.patiencecount += 1\n            #print(f\"Early stopping counter: {self.patiencecount} out of {patience}\")\n            if self.patiencecount >= patience:\n                return True\n        return False\n\n      def fit(self, train_dl, val_dl, epochs, pad_idx = 0):\n        optimizer = torch.optim.Adam(self.parameters(), lr = 0.001)\n\n        self.patiencecount = 0\n        self.min_validation_loss = float('inf')\n\n        self.trainlosses = []\n        self.vallosses = []\n\n        self.train_accs = []\n        self.val_accs = []\n\n        counter = 0\n        for epoch in range(epochs):\n          for batch in train_dl:\n            counter += 1\n            text = batch[0]\n\n            output = batch[1]\n            output = output.transpose(0,1)\n            tags = output[0]\n            mask = output[1]\n            optimizer.zero_grad()\n            predictions = self.forward(text)\n            predictions = predictions.transpose(1,2)\n            loss = self.loss_fn(predictions, tags)\n            loss = (loss * mask).sum() / mask.sum()\n            loss.backward()\n            optimizer.step()\n          train_loss, train_acc = self.evaluate(train_dl)\n          val_loss, val_acc = self.evaluate(val_dl)\n          self.trainlosses.append(train_loss)\n          self.vallosses.append(val_loss)\n          self.train_accs.append(train_acc)\n          self.val_accs.append(val_acc)\n          if self.early_stop(val_loss):\n            break\n          train_acc = train_acc*100\n          val_acc = val_acc*100\n          #print(f\"Epoch [{epoch + 1}/{epochs}] - TrainLoss: {train_loss:.4f}, ValLoss: {val_loss:.4f}, TrainAcc: {train_acc:.2f},% ValAcc: {val_acc:.2f}%\")\n\n      def evaluate(self, val_dl, pad_idx = 0):\n        losses = []\n        accuracies = []\n        with torch.no_grad():\n          for batch in val_dl:\n              text = batch[0]\n              output = batch[1]\n              output = output.transpose(0,1)\n              tags = output[0]\n              mask = output[1]\n              predictions = self.forward(text)\n              predictions = predictions.transpose(1,2)\n              loss = self.loss_fn(predictions, tags)\n              loss = (loss * mask).sum() / mask.sum()\n              losses.append(loss)\n              acc = self.categorical_accuracy(predictions, tags, mask, pad_idx)\n              accuracies.append(acc)\n        return torch.Tensor(losses).mean(), torch.Tensor(accuracies).mean()\n\n\n    class OHGTagger(Model):\n        def __init__(self, vocab, embeds, hidden, tagset, n_layers, dropout, criterion, pad_idx = 0):\n            super().__init__()\n\n            self.embedding = nn.Embedding(vocab, embeds, padding_idx = pad_idx, scale_grad_by_freq = True)\n\n            self.lstm = nn.LSTM(embeds, hidden, num_layers = n_layers, bidirectional = True,\n                                dropout = dropout if n_layers > 1 else 0)\n\n            self.lin = nn.Linear(hidden * 2, tagset)\n\n            self.dropout = nn.Dropout(dropout)\n\n            self.loss_fn = criterion\n\n        def forward(self, sent):\n            x = self.embedding(sent)\n            x, (hidden, cell) = self.lstm(x)\n            x = self.lin(self.dropout(x))\n            return x\n\n    vocab = len(allwords)+1\n    n_emb = 300\n    n_hidden = 60\n    n_tags = len(alltags)+1\n    n_layers = 3\n    dropout = .2\n    pad_idx = 0\n    criterion = nn.CrossEntropyLoss(reduction = 'none', weight = class_weights, ignore_index = pad_idx).to(device)\n\n    network_OHGTagger = OHGTagger(vocab, n_emb, n_hidden, n_tags, n_layers, dropout, criterion, pad_idx).to(device)\n\n    %time network_OHGTagger.fit(train_loader, test_loader, 100)\n\n    preds, tags, texts = predict(network_OHGTagger, test_loader, pad_idx)\n\n    for i in range(len(preds)):\n      for j in range(len(preds[i])):\n        if tags[i][j] == 0:\n          preds[i][j] = 0\n      preds[i] = [ele for ele in preds[i] if ele != 0]\n      tags[i] = [ele for ele in tags[i] if ele != 0]\n      texts[i] = [ele for ele in texts[i] if ele != 0]\n    decpreds, dectags, dectext, declangs = [], [], [], []\n    decpreds.append(tokenize(preds, tag_decoder))\n    dectags.append(tokenize(tags, tag_decoder))\n    dectext.append(tokenize(texts, word_decoder))\n    decpreds = decpreds[0]\n    dectags = dectags[0]\n    dectext = dectext[0]\n\n    in_set_vocab = []\n    for batch in train_loader:\n      texts = batch[0]\n      for sent in texts:\n        sent = sent.tolist()\n        sent = [ele for ele in sent if ele != 0]\n        x = []\n        for i in sent:\n          x.append([i])\n        x = tokenize(x, word_decoder)\n        for word in x:\n          in_set_vocab.append(word)\n    in_set_vocab = set([x for y in in_set_vocab for x in y])\n\n    corrects, incorrects, totals = [], [], []\n    uk_corrects, uk_incorrects, uk_totals = [], [], []\n    for i in range(len(decpreds)):\n      for j in range(len(decpreds[i])):\n        totals.append(dectags[i][j])\n        if decpreds[i][j] == dectags[i][j]:\n          corrects.append(decpreds[i][j])\n        else:\n          incorrects.append(decpreds[i][j])\n        if dectext[i][j] not in in_set_vocab:\n          uk_totals.append(dectags[i][j])\n          if decpreds[i][j] == dectags[i][j]:\n            uk_corrects.append(decpreds[i][j])\n          else:\n            uk_incorrects.append(decpreds[i][j])\n\n    c_freqs = Counter(corrects)\n    ic_freqs = Counter(incorrects)\n    t_freqs = Counter(totals)\n    uk_c_freqs = Counter(uk_corrects)\n    uk_ic_freqs = Counter(uk_incorrects)\n    uk_t_freqs = Counter(uk_totals)\n\n    data = []\n    labels = []\n    outs = []\n    any_c = [x[0] for x in c_freqs.most_common()]\n    for i in t_freqs.most_common():\n      if i[0] not in any_c:\n        data.append(0)\n        labels.append(i[0])\n        outs.append((i[0], 0))\n        results_df.loc[len(results_df)] = [i[0], 0, size]\n      else:\n        for ele in c_freqs.most_common():\n          if ele[0] == i[0]:\n            num_c = ele[1]\n        class_acc = num_c/i[1]\n        data.append(class_acc*100)\n        labels.append(i[0])\n        outs.append((i[0], class_acc*100))\n        results_df.loc[len(results_df)] = [i[0], class_acc*100, size]\n\n  results_df.to_csv('resultsOHG.csv', index=False)\n  with torch.no_grad():\n      torch.cuda.empty_cache()\n  iteration = iteration + 1\n------------------\n\n----- stdout -----\n0\n------------------\n\n\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)\nCell \u001b[1;32mIn[14], line 73\u001b[0m\n\u001b[0;32m     71\u001b[0m mask \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(seq)\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, tag \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(seq):\n\u001b[1;32m---> 73\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m tag \u001b[38;5;241m==\u001b[39m \u001b[43mtag_tokenizer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mXX\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m:\n\u001b[0;32m     74\u001b[0m     mask[idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     75\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m tag \u001b[38;5;241m==\u001b[39m tag_tokenizer[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\n\u001b[1;31mKeyError\u001b[0m: 'XX'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCellExecutionError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m     nb_in \u001b[38;5;241m=\u001b[39m nbformat\u001b[38;5;241m.\u001b[39mread(ff, nbformat\u001b[38;5;241m.\u001b[39mNO_CONVERT)\n\u001b[0;32m      5\u001b[0m ep \u001b[38;5;241m=\u001b[39m ExecutePreprocessor(kernel_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpython3\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m nb_out \u001b[38;5;241m=\u001b[39m \u001b[43mep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnb_in\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\cjack\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nbconvert\\preprocessors\\execute.py:103\u001b[0m, in \u001b[0;36mExecutePreprocessor.preprocess\u001b[1;34m(self, nb, resources, km)\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnb\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlanguage_info\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m info_msg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlanguage_info\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m index, cell \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnb\u001b[38;5;241m.\u001b[39mcells):\n\u001b[1;32m--> 103\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess_cell\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcell\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresources\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_widgets_metadata()\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnb, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresources\n",
      "File \u001b[1;32mc:\\Users\\cjack\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nbconvert\\preprocessors\\execute.py:124\u001b[0m, in \u001b[0;36mExecutePreprocessor.preprocess_cell\u001b[1;34m(self, cell, resources, index)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;124;03mOverride if you want to apply some preprocessing to each cell.\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03mMust return modified cell and resource dictionary.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;124;03m    Index of the cell being processed\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_assign_resources(resources)\n\u001b[1;32m--> 124\u001b[0m cell \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_cell\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcell\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore_history\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cell, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresources\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\jupyter_core\\utils\\__init__.py:159\u001b[0m, in \u001b[0;36mrun_sync.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _runner_map:\n\u001b[0;32m    158\u001b[0m         _runner_map[name] \u001b[38;5;241m=\u001b[39m _TaskRunner()\n\u001b[1;32m--> 159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_runner_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43minner\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m:\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\jupyter_core\\utils\\__init__.py:126\u001b[0m, in \u001b[0;36m_TaskRunner.run\u001b[1;34m(self, coro)\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__runner_thread\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m    125\u001b[0m fut \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mrun_coroutine_threadsafe(coro, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__io_loop)\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\cjack\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:456\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    458\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[1;32mc:\\Users\\cjack\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\cjack\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nbclient\\client.py:1062\u001b[0m, in \u001b[0;36mNotebookClient.async_execute_cell\u001b[1;34m(self, cell, cell_index, execution_count, store_history)\u001b[0m\n\u001b[0;32m   1058\u001b[0m                 new_outputs\u001b[38;5;241m.\u001b[39minsert(i, stdout)\n\u001b[0;32m   1060\u001b[0m     cell\u001b[38;5;241m.\u001b[39moutputs \u001b[38;5;241m=\u001b[39m new_outputs\n\u001b[1;32m-> 1062\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_raise_for_error(cell, cell_index, exec_reply)\n\u001b[0;32m   1064\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnb[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcells\u001b[39m\u001b[38;5;124m\"\u001b[39m][cell_index] \u001b[38;5;241m=\u001b[39m cell\n\u001b[0;32m   1065\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cell\n",
      "File \u001b[1;32mc:\\Users\\cjack\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nbclient\\client.py:918\u001b[0m, in \u001b[0;36mNotebookClient._check_raise_for_error\u001b[1;34m(self, cell, cell_index, exec_reply)\u001b[0m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m run_hook(\n\u001b[0;32m    915\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_cell_error, cell\u001b[38;5;241m=\u001b[39mcell, cell_index\u001b[38;5;241m=\u001b[39mcell_index, execute_reply\u001b[38;5;241m=\u001b[39mexec_reply\n\u001b[0;32m    916\u001b[0m )\n\u001b[0;32m    917\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cell_allows_errors:\n\u001b[1;32m--> 918\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CellExecutionError\u001b[38;5;241m.\u001b[39mfrom_cell_and_msg(cell, exec_reply_content)\n",
      "\u001b[1;31mCellExecutionError\u001b[0m: An error occurred while executing the following cell:\n------------------\niteration = 0\nwhile iteration < 100:\n  print(iteration)\n  for size in sample_sizes:\n    random_indices = random.sample(range(len(base_sents)), size)\n    sents = []\n    tags = []\n    for index in random_indices:\n      sents.append(base_sents[index])\n      tags.append(base_tags[index])\n    for idx, sent in enumerate(sents):\n      sent.insert(0, 'start')\n      sent.append('stop')\n      sents[idx] = sent\n    for idx, sent in enumerate(tags):\n      sent.insert(0, 'start')\n      sent.append('stop')\n      tags[idx] = sent\n    rawwords = []\n    for sent in sents:\n      for word in sent:\n        rawwords.append(word)\n\n    rawtags = []\n    for sequence in tags:\n      for tag in sequence:\n        rawtags.append(tag)\n\n    allwords = list(set(rawwords))\n    alltags = list(set(rawtags))\n\n    word_tokenizer = {word: idx+1 for idx, word in enumerate(allwords)}\n    word_decoder = {idx+1: word for idx, word in enumerate(allwords)}\n    tag_tokenizer = {tag: idx+1 for idx, tag in enumerate(alltags)}\n    tag_decoder = {idx+1: tag for idx, tag in enumerate(alltags)}\n\n    def tokenize(sentences, tokenizer):\n      indexed_sentences = []\n      for sentence in sentences:\n        indexed_sentence = [tokenizer[word] for word in sentence]\n        indexed_sentences.append(indexed_sentence)\n      return indexed_sentences\n\n    encsents = tokenize(sents, word_tokenizer)\n    enctags = tokenize(tags, tag_tokenizer)\n\n    padsents, padtags = [], []\n\n    maxlen = max(len(sublist) for sublist in encsents)\n    for sublist in encsents:\n      while len(sublist) < maxlen:\n        sublist = sublist + [0]\n        if len(sublist) == maxlen:\n          break\n        sublist = [0] + sublist\n      padsents.append(sublist)\n    allenctags = []\n    maxlen = max(len(sublist) for sublist in enctags)\n    for sublist in enctags:\n      for i in sublist:\n        allenctags.append(i)\n      while len(sublist) < maxlen:\n        sublist = sublist + [0]\n        if len(sublist) == maxlen:\n          break\n        sublist = [0] + sublist\n      padtags.append(sublist)\n\n    tag_mask = []\n    for seq in padtags:\n      mask = [1]*len(seq)\n      for idx, tag in enumerate(seq):\n        if tag == tag_tokenizer['XX']:\n          mask[idx] = 0\n        if tag == tag_tokenizer['start']:\n          mask[idx] = 0\n        if tag == tag_tokenizer['stop']:\n          mask[idx] = 0\n        if tag == tag_tokenizer['PUNCT']:\n          mask[idx] = 0\n      tag_mask.append(mask)\n    for i in range(len(tag_mask)):\n      if len(tag_mask[i]) != len(padsents[i]):\n        print(i)\n    pad_tags_mask = []\n    for i in range(len(padtags)):\n      pad_tags_mask.append([padtags[i], tag_mask[i]])\n    X_test, X_train, y_test, y_train = train_test_split(padsents, pad_tags_mask, test_size=0.8)\n    X_train_tensor = torch.tensor(X_train, dtype=torch.long).to(device)\n    y_train_tensor = torch.tensor(y_train, dtype=torch.long).to(device)\n    X_test_tensor = torch.tensor(X_test, dtype=torch.long).to(device)\n    y_test_tensor = torch.tensor(y_test, dtype=torch.long).to(device)\n    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n    batch_size = 64\n    train_loader = DataLoader(train_dataset, batch_size=batch_size)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n\n    class_weights = calculate_class_weights(allenctags)\n\n    class Model(nn.Module):\n      def __init__(self):\n        super().__init__()\n\n      def categorical_accuracy(self, preds, y, mask, tag_pad_idx = 0):\n        max_preds = preds.argmax(dim = 1, keepdim = False)\n        max_preds = max_preds*mask\n        y = y*mask\n        max_preds = torch.flatten(max_preds)\n        y = torch.flatten(y)\n        non_pad_elements = y.nonzero()\n        correct = max_preds[non_pad_elements].eq(y[non_pad_elements])\n        return correct.sum() / y[non_pad_elements].shape[0]\n\n      def early_stop(self, validation_loss, patience = 3, min_delta = 0):\n        if validation_loss < self.min_validation_loss:\n            self.min_validation_loss = validation_loss\n            self.patiencecount = 0\n        elif validation_loss > (self.min_validation_loss + min_delta):\n            self.patiencecount += 1\n            #print(f\"Early stopping counter: {self.patiencecount} out of {patience}\")\n            if self.patiencecount >= patience:\n                return True\n        return False\n\n      def fit(self, train_dl, val_dl, epochs, pad_idx = 0):\n        optimizer = torch.optim.Adam(self.parameters(), lr = 0.001)\n\n        self.patiencecount = 0\n        self.min_validation_loss = float('inf')\n\n        self.trainlosses = []\n        self.vallosses = []\n\n        self.train_accs = []\n        self.val_accs = []\n\n        counter = 0\n        for epoch in range(epochs):\n          for batch in train_dl:\n            counter += 1\n            text = batch[0]\n\n            output = batch[1]\n            output = output.transpose(0,1)\n            tags = output[0]\n            mask = output[1]\n            optimizer.zero_grad()\n            predictions = self.forward(text)\n            predictions = predictions.transpose(1,2)\n            loss = self.loss_fn(predictions, tags)\n            loss = (loss * mask).sum() / mask.sum()\n            loss.backward()\n            optimizer.step()\n          train_loss, train_acc = self.evaluate(train_dl)\n          val_loss, val_acc = self.evaluate(val_dl)\n          self.trainlosses.append(train_loss)\n          self.vallosses.append(val_loss)\n          self.train_accs.append(train_acc)\n          self.val_accs.append(val_acc)\n          if self.early_stop(val_loss):\n            break\n          train_acc = train_acc*100\n          val_acc = val_acc*100\n          #print(f\"Epoch [{epoch + 1}/{epochs}] - TrainLoss: {train_loss:.4f}, ValLoss: {val_loss:.4f}, TrainAcc: {train_acc:.2f},% ValAcc: {val_acc:.2f}%\")\n\n      def evaluate(self, val_dl, pad_idx = 0):\n        losses = []\n        accuracies = []\n        with torch.no_grad():\n          for batch in val_dl:\n              text = batch[0]\n              output = batch[1]\n              output = output.transpose(0,1)\n              tags = output[0]\n              mask = output[1]\n              predictions = self.forward(text)\n              predictions = predictions.transpose(1,2)\n              loss = self.loss_fn(predictions, tags)\n              loss = (loss * mask).sum() / mask.sum()\n              losses.append(loss)\n              acc = self.categorical_accuracy(predictions, tags, mask, pad_idx)\n              accuracies.append(acc)\n        return torch.Tensor(losses).mean(), torch.Tensor(accuracies).mean()\n\n\n    class OHGTagger(Model):\n        def __init__(self, vocab, embeds, hidden, tagset, n_layers, dropout, criterion, pad_idx = 0):\n            super().__init__()\n\n            self.embedding = nn.Embedding(vocab, embeds, padding_idx = pad_idx, scale_grad_by_freq = True)\n\n            self.lstm = nn.LSTM(embeds, hidden, num_layers = n_layers, bidirectional = True,\n                                dropout = dropout if n_layers > 1 else 0)\n\n            self.lin = nn.Linear(hidden * 2, tagset)\n\n            self.dropout = nn.Dropout(dropout)\n\n            self.loss_fn = criterion\n\n        def forward(self, sent):\n            x = self.embedding(sent)\n            x, (hidden, cell) = self.lstm(x)\n            x = self.lin(self.dropout(x))\n            return x\n\n    vocab = len(allwords)+1\n    n_emb = 300\n    n_hidden = 60\n    n_tags = len(alltags)+1\n    n_layers = 3\n    dropout = .2\n    pad_idx = 0\n    criterion = nn.CrossEntropyLoss(reduction = 'none', weight = class_weights, ignore_index = pad_idx).to(device)\n\n    network_OHGTagger = OHGTagger(vocab, n_emb, n_hidden, n_tags, n_layers, dropout, criterion, pad_idx).to(device)\n\n    %time network_OHGTagger.fit(train_loader, test_loader, 100)\n\n    preds, tags, texts = predict(network_OHGTagger, test_loader, pad_idx)\n\n    for i in range(len(preds)):\n      for j in range(len(preds[i])):\n        if tags[i][j] == 0:\n          preds[i][j] = 0\n      preds[i] = [ele for ele in preds[i] if ele != 0]\n      tags[i] = [ele for ele in tags[i] if ele != 0]\n      texts[i] = [ele for ele in texts[i] if ele != 0]\n    decpreds, dectags, dectext, declangs = [], [], [], []\n    decpreds.append(tokenize(preds, tag_decoder))\n    dectags.append(tokenize(tags, tag_decoder))\n    dectext.append(tokenize(texts, word_decoder))\n    decpreds = decpreds[0]\n    dectags = dectags[0]\n    dectext = dectext[0]\n\n    in_set_vocab = []\n    for batch in train_loader:\n      texts = batch[0]\n      for sent in texts:\n        sent = sent.tolist()\n        sent = [ele for ele in sent if ele != 0]\n        x = []\n        for i in sent:\n          x.append([i])\n        x = tokenize(x, word_decoder)\n        for word in x:\n          in_set_vocab.append(word)\n    in_set_vocab = set([x for y in in_set_vocab for x in y])\n\n    corrects, incorrects, totals = [], [], []\n    uk_corrects, uk_incorrects, uk_totals = [], [], []\n    for i in range(len(decpreds)):\n      for j in range(len(decpreds[i])):\n        totals.append(dectags[i][j])\n        if decpreds[i][j] == dectags[i][j]:\n          corrects.append(decpreds[i][j])\n        else:\n          incorrects.append(decpreds[i][j])\n        if dectext[i][j] not in in_set_vocab:\n          uk_totals.append(dectags[i][j])\n          if decpreds[i][j] == dectags[i][j]:\n            uk_corrects.append(decpreds[i][j])\n          else:\n            uk_incorrects.append(decpreds[i][j])\n\n    c_freqs = Counter(corrects)\n    ic_freqs = Counter(incorrects)\n    t_freqs = Counter(totals)\n    uk_c_freqs = Counter(uk_corrects)\n    uk_ic_freqs = Counter(uk_incorrects)\n    uk_t_freqs = Counter(uk_totals)\n\n    data = []\n    labels = []\n    outs = []\n    any_c = [x[0] for x in c_freqs.most_common()]\n    for i in t_freqs.most_common():\n      if i[0] not in any_c:\n        data.append(0)\n        labels.append(i[0])\n        outs.append((i[0], 0))\n        results_df.loc[len(results_df)] = [i[0], 0, size]\n      else:\n        for ele in c_freqs.most_common():\n          if ele[0] == i[0]:\n            num_c = ele[1]\n        class_acc = num_c/i[1]\n        data.append(class_acc*100)\n        labels.append(i[0])\n        outs.append((i[0], class_acc*100))\n        results_df.loc[len(results_df)] = [i[0], class_acc*100, size]\n\n  results_df.to_csv('resultsOHG.csv', index=False)\n  with torch.no_grad():\n      torch.cuda.empty_cache()\n  iteration = iteration + 1\n------------------\n\n----- stdout -----\n0\n------------------\n\n\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)\nCell \u001b[1;32mIn[14], line 73\u001b[0m\n\u001b[0;32m     71\u001b[0m mask \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(seq)\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, tag \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(seq):\n\u001b[1;32m---> 73\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m tag \u001b[38;5;241m==\u001b[39m \u001b[43mtag_tokenizer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mXX\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m:\n\u001b[0;32m     74\u001b[0m     mask[idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     75\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m tag \u001b[38;5;241m==\u001b[39m tag_tokenizer[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\n\u001b[1;31mKeyError\u001b[0m: 'XX'\n"
     ]
    }
   ],
   "source": [
    "filename = 'AblationStudyOHG.ipynb'\n",
    "with open(filename) as ff:\n",
    "    nb_in = nbformat.read(ff, nbformat.NO_CONVERT)\n",
    "    \n",
    "ep = ExecutePreprocessor(kernel_name='python3')\n",
    "\n",
    "nb_out = ep.preprocess(nb_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbformat\n",
    "from nbconvert.preprocessors import ExecutePreprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'AblationStudyOI.ipynb'\n",
    "with open(filename) as ff:\n",
    "    nb_in = nbformat.read(ff, nbformat.NO_CONVERT)\n",
    "    \n",
    "ep = ExecutePreprocessor(kernel_name='python3')\n",
    "\n",
    "nb_out = ep.preprocess(nb_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbformat\n",
    "from nbconvert.preprocessors import ExecutePreprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'AblationStudyOS.ipynb'\n",
    "with open(filename) as ff:\n",
    "    nb_in = nbformat.read(ff, nbformat.NO_CONVERT)\n",
    "    \n",
    "ep = ExecutePreprocessor(kernel_name='python3')\n",
    "\n",
    "nb_out = ep.preprocess(nb_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbformat\n",
    "from nbconvert.preprocessors import ExecutePreprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'AblationStudyGOTH.ipynb'\n",
    "with open(filename) as ff:\n",
    "    nb_in = nbformat.read(ff, nbformat.NO_CONVERT)\n",
    "    \n",
    "ep = ExecutePreprocessor(kernel_name='python3')\n",
    "\n",
    "nb_out = ep.preprocess(nb_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
