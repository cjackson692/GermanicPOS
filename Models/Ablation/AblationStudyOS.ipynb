{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 22676,
     "status": "ok",
     "timestamp": 1740715060512,
     "user": {
      "displayName": "Carter Smith",
      "userId": "16068926545987539689"
     },
     "user_tz": 360
    },
    "id": "JLWCPXV7bpnu"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from collections import Counter\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 99,
     "status": "ok",
     "timestamp": 1740715060515,
     "user": {
      "displayName": "Carter Smith",
      "userId": "16068926545987539689"
     },
     "user_tz": 360
    },
    "id": "mjKmP6NjbrVv",
    "outputId": "207fb63b-b0c9-46c3-a14c-b199052df6df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script is running with GPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "  device = torch.device(\"cuda\")\n",
    "  print('Script is running with GPU')\n",
    "else:\n",
    "  device = torch.device(\"cpu\")\n",
    "  print('Script is running WITHOUT GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_class_weights(y):\n",
    "    unique_classes, class_counts = np.unique(y, return_counts=True)\n",
    "    total_samples = len(y)\n",
    "    class_weights = []\n",
    "    class_weights.append(1)\n",
    "\n",
    "    for class_label, class_count in zip(unique_classes, class_counts):\n",
    "        class_weight = math.log(total_samples / (class_count))\n",
    "        class_weights.append(class_weight)\n",
    "\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "    return class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1740715060517,
     "user": {
      "displayName": "Carter Smith",
      "userId": "16068926545987539689"
     },
     "user_tz": 360
    },
    "id": "OFG0VTkBeHRO"
   },
   "outputs": [],
   "source": [
    "def predict(model, data, pad_idx):\n",
    "    with torch.no_grad():\n",
    "      predicts_out = []\n",
    "      tags_out = []\n",
    "      text_out = []\n",
    "      for batch in data:\n",
    "        text = batch[0]\n",
    "        output = batch[1]\n",
    "        output = output.transpose(0,1)\n",
    "        tags = output[0]\n",
    "        mask = output[1]\n",
    "        predictions = model.forward(text)\n",
    "        predictions = predictions.transpose(1,2)\n",
    "        predictions = predictions.argmax(dim = 1)\n",
    "        for idx, sent in enumerate(text):\n",
    "          predicts_out.append(np.asarray(predictions[idx].cpu()))\n",
    "          tags_out.append(np.asarray(tags[idx].cpu()))\n",
    "          text_out.append(np.asarray(sent.cpu()))\n",
    "    for idx, ele in enumerate(predicts_out):\n",
    "      predicts_out[idx] = predicts_out[idx].tolist()\n",
    "    for idx, ele in enumerate(tags_out):\n",
    "      tags_out[idx] = tags_out[idx].tolist()\n",
    "    for idx, ele in enumerate(text_out):\n",
    "      text_out[idx] = text_out[idx].tolist()\n",
    "\n",
    "    return predicts_out, tags_out, text_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 903,
     "status": "ok",
     "timestamp": 1740715061413,
     "user": {
      "displayName": "Carter Smith",
      "userId": "16068926545987539689"
     },
     "user_tz": 360
    },
    "id": "Dzjjb9j_b61c"
   },
   "outputs": [],
   "source": [
    "base_sents = list(np.load('OS_sents.npy', allow_pickle=True))\n",
    "base_tags = list(np.load('OS_tags_minimal.npy', allow_pickle=True))\n",
    "problems = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1740715061509,
     "user": {
      "displayName": "Carter Smith",
      "userId": "16068926545987539689"
     },
     "user_tz": 360
    },
    "id": "hqD-nAXPb3mb"
   },
   "outputs": [],
   "source": [
    "sample_sizes = [250, 500, 750, 1000, 1500, 2000, 2500, 3000, 3549]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1740715061534,
     "user": {
      "displayName": "Carter Smith",
      "userId": "16068926545987539689"
     },
     "user_tz": 360
    },
    "id": "p7XDSAldcfdy"
   },
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(columns=[\"tag\", \"accuracy\", \"sample_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1740715062182,
     "user": {
      "displayName": "Carter Smith",
      "userId": "16068926545987539689"
     },
     "user_tz": 360
    },
    "id": "LT4QGOvzy-RO",
    "outputId": "ffca14fc-3334-4616-a273-c3e84bdee4e5"
   },
   "outputs": [],
   "source": [
    "results_df = pd.read_csv('resultsOS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "2ab1COwJbug4",
    "outputId": "3ad37e16-556e-47cd-a38a-e68d20e23dc5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "CPU times: total: 391 ms\n",
      "Wall time: 2.13 s\n",
      "CPU times: total: 31.2 ms\n",
      "Wall time: 1.06 s\n",
      "CPU times: total: 203 ms\n",
      "Wall time: 1.36 s\n",
      "CPU times: total: 281 ms\n",
      "Wall time: 1.42 s\n",
      "CPU times: total: 609 ms\n",
      "Wall time: 1.82 s\n",
      "CPU times: total: 688 ms\n",
      "Wall time: 1.92 s\n",
      "CPU times: total: 656 ms\n",
      "Wall time: 2.27 s\n",
      "CPU times: total: 609 ms\n",
      "Wall time: 2.16 s\n",
      "CPU times: total: 781 ms\n",
      "Wall time: 2.55 s\n",
      "1\n",
      "CPU times: total: 15.6 ms\n",
      "Wall time: 762 ms\n",
      "CPU times: total: 46.9 ms\n",
      "Wall time: 1.04 s\n",
      "CPU times: total: 297 ms\n",
      "Wall time: 1.15 s\n",
      "CPU times: total: 266 ms\n",
      "Wall time: 1.15 s\n",
      "CPU times: total: 375 ms\n",
      "Wall time: 1.66 s\n",
      "CPU times: total: 453 ms\n",
      "Wall time: 1.79 s\n",
      "CPU times: total: 656 ms\n",
      "Wall time: 2.12 s\n",
      "CPU times: total: 891 ms\n",
      "Wall time: 2.59 s\n",
      "CPU times: total: 922 ms\n",
      "Wall time: 2.84 s\n",
      "2\n",
      "CPU times: total: 31.2 ms\n",
      "Wall time: 800 ms\n",
      "CPU times: total: 31.2 ms\n",
      "Wall time: 993 ms\n",
      "CPU times: total: 312 ms\n",
      "Wall time: 1.22 s\n",
      "CPU times: total: 328 ms\n",
      "Wall time: 1.36 s\n",
      "CPU times: total: 484 ms\n",
      "Wall time: 1.68 s\n",
      "CPU times: total: 453 ms\n",
      "Wall time: 2.06 s\n",
      "CPU times: total: 562 ms\n",
      "Wall time: 2.08 s\n",
      "CPU times: total: 672 ms\n",
      "Wall time: 2.45 s\n",
      "CPU times: total: 844 ms\n",
      "Wall time: 2.61 s\n",
      "3\n",
      "CPU times: total: 15.6 ms\n",
      "Wall time: 855 ms\n",
      "CPU times: total: 15.6 ms\n",
      "Wall time: 1.04 s\n",
      "CPU times: total: 328 ms\n",
      "Wall time: 1.18 s\n",
      "CPU times: total: 328 ms\n",
      "Wall time: 1.4 s\n",
      "CPU times: total: 516 ms\n",
      "Wall time: 1.67 s\n",
      "CPU times: total: 344 ms\n",
      "Wall time: 1.59 s\n",
      "CPU times: total: 672 ms\n",
      "Wall time: 2.01 s\n",
      "CPU times: total: 719 ms\n",
      "Wall time: 2.53 s\n",
      "CPU times: total: 781 ms\n",
      "Wall time: 2.52 s\n",
      "4\n",
      "CPU times: total: 15.6 ms\n",
      "Wall time: 818 ms\n",
      "CPU times: total: 78.1 ms\n",
      "Wall time: 986 ms\n",
      "CPU times: total: 328 ms\n",
      "Wall time: 1.42 s\n",
      "CPU times: total: 297 ms\n",
      "Wall time: 1.56 s\n",
      "CPU times: total: 344 ms\n",
      "Wall time: 1.5 s\n",
      "CPU times: total: 547 ms\n",
      "Wall time: 2.1 s\n",
      "CPU times: total: 547 ms\n",
      "Wall time: 1.91 s\n",
      "CPU times: total: 578 ms\n",
      "Wall time: 2.61 s\n",
      "CPU times: total: 844 ms\n",
      "Wall time: 2.78 s\n",
      "5\n",
      "CPU times: total: 62.5 ms\n",
      "Wall time: 754 ms\n",
      "CPU times: total: 188 ms\n",
      "Wall time: 975 ms\n",
      "CPU times: total: 344 ms\n",
      "Wall time: 1.42 s\n",
      "CPU times: total: 375 ms\n",
      "Wall time: 1.5 s\n",
      "CPU times: total: 359 ms\n",
      "Wall time: 2.01 s\n",
      "CPU times: total: 516 ms\n",
      "Wall time: 2.32 s\n",
      "CPU times: total: 766 ms\n",
      "Wall time: 2.16 s\n",
      "CPU times: total: 828 ms\n",
      "Wall time: 2.69 s\n",
      "CPU times: total: 781 ms\n",
      "Wall time: 2.76 s\n",
      "6\n",
      "CPU times: total: 46.9 ms\n",
      "Wall time: 971 ms\n",
      "CPU times: total: 46.9 ms\n",
      "Wall time: 1.12 s\n",
      "CPU times: total: 281 ms\n",
      "Wall time: 1.12 s\n",
      "CPU times: total: 281 ms\n",
      "Wall time: 1.35 s\n",
      "CPU times: total: 391 ms\n",
      "Wall time: 1.51 s\n",
      "CPU times: total: 766 ms\n",
      "Wall time: 2.29 s\n",
      "CPU times: total: 609 ms\n",
      "Wall time: 2.16 s\n",
      "CPU times: total: 781 ms\n",
      "Wall time: 2.31 s\n",
      "CPU times: total: 938 ms\n",
      "Wall time: 2.66 s\n",
      "7\n",
      "CPU times: total: 15.6 ms\n",
      "Wall time: 848 ms\n",
      "CPU times: total: 31.2 ms\n",
      "Wall time: 1.01 s\n",
      "CPU times: total: 281 ms\n",
      "Wall time: 1.36 s\n",
      "CPU times: total: 188 ms\n",
      "Wall time: 1.29 s\n",
      "CPU times: total: 391 ms\n",
      "Wall time: 1.65 s\n",
      "CPU times: total: 766 ms\n",
      "Wall time: 2.07 s\n",
      "CPU times: total: 750 ms\n",
      "Wall time: 2.33 s\n",
      "CPU times: total: 656 ms\n",
      "Wall time: 2.65 s\n",
      "CPU times: total: 906 ms\n",
      "Wall time: 3.11 s\n",
      "8\n",
      "CPU times: total: 31.2 ms\n",
      "Wall time: 811 ms\n",
      "CPU times: total: 31.2 ms\n",
      "Wall time: 1.13 s\n",
      "CPU times: total: 266 ms\n",
      "Wall time: 1.44 s\n",
      "CPU times: total: 344 ms\n",
      "Wall time: 1.39 s\n",
      "CPU times: total: 531 ms\n",
      "Wall time: 1.79 s\n",
      "CPU times: total: 578 ms\n",
      "Wall time: 2.13 s\n",
      "CPU times: total: 812 ms\n",
      "Wall time: 2.63 s\n",
      "CPU times: total: 797 ms\n",
      "Wall time: 2.51 s\n",
      "CPU times: total: 859 ms\n",
      "Wall time: 2.84 s\n",
      "9\n",
      "CPU times: total: 62.5 ms\n",
      "Wall time: 951 ms\n",
      "CPU times: total: 62.5 ms\n",
      "Wall time: 1.08 s\n",
      "CPU times: total: 344 ms\n",
      "Wall time: 1.17 s\n",
      "CPU times: total: 266 ms\n",
      "Wall time: 1.5 s\n",
      "CPU times: total: 422 ms\n",
      "Wall time: 1.78 s\n",
      "CPU times: total: 656 ms\n",
      "Wall time: 1.93 s\n",
      "CPU times: total: 688 ms\n",
      "Wall time: 2.66 s\n",
      "CPU times: total: 828 ms\n",
      "Wall time: 2.71 s\n",
      "CPU times: total: 922 ms\n",
      "Wall time: 3.32 s\n",
      "10\n",
      "CPU times: total: 15.6 ms\n",
      "Wall time: 911 ms\n",
      "CPU times: total: 46.9 ms\n",
      "Wall time: 1.16 s\n",
      "CPU times: total: 312 ms\n",
      "Wall time: 1.49 s\n",
      "CPU times: total: 312 ms\n",
      "Wall time: 1.68 s\n",
      "CPU times: total: 516 ms\n",
      "Wall time: 1.77 s\n",
      "CPU times: total: 703 ms\n",
      "Wall time: 2.19 s\n",
      "CPU times: total: 641 ms\n",
      "Wall time: 2.61 s\n",
      "CPU times: total: 1.11 s\n",
      "Wall time: 2.93 s\n",
      "CPU times: total: 734 ms\n",
      "Wall time: 2.5 s\n",
      "11\n",
      "CPU times: total: 15.6 ms\n",
      "Wall time: 914 ms\n",
      "CPU times: total: 93.8 ms\n",
      "Wall time: 1.17 s\n",
      "CPU times: total: 328 ms\n",
      "Wall time: 1.3 s\n",
      "CPU times: total: 250 ms\n",
      "Wall time: 1.31 s\n",
      "CPU times: total: 562 ms\n",
      "Wall time: 1.88 s\n",
      "CPU times: total: 781 ms\n",
      "Wall time: 2.28 s\n",
      "CPU times: total: 781 ms\n",
      "Wall time: 2.34 s\n",
      "CPU times: total: 844 ms\n",
      "Wall time: 2.57 s\n",
      "CPU times: total: 828 ms\n",
      "Wall time: 2.54 s\n",
      "12\n",
      "CPU times: total: 78.1 ms\n",
      "Wall time: 868 ms\n",
      "CPU times: total: 62.5 ms\n",
      "Wall time: 1.13 s\n",
      "CPU times: total: 344 ms\n",
      "Wall time: 1.47 s\n",
      "CPU times: total: 250 ms\n",
      "Wall time: 1.5 s\n",
      "CPU times: total: 266 ms\n",
      "Wall time: 1.55 s\n",
      "CPU times: total: 750 ms\n",
      "Wall time: 2.24 s\n",
      "CPU times: total: 750 ms\n",
      "Wall time: 2.27 s\n",
      "CPU times: total: 1.05 s\n",
      "Wall time: 2.95 s\n",
      "CPU times: total: 969 ms\n",
      "Wall time: 2.82 s\n",
      "13\n",
      "CPU times: total: 15.6 ms\n",
      "Wall time: 899 ms\n",
      "CPU times: total: 62.5 ms\n",
      "Wall time: 1.18 s\n",
      "CPU times: total: 359 ms\n",
      "Wall time: 1.3 s\n",
      "CPU times: total: 281 ms\n",
      "Wall time: 1.56 s\n",
      "CPU times: total: 375 ms\n",
      "Wall time: 1.84 s\n",
      "CPU times: total: 641 ms\n",
      "Wall time: 1.91 s\n",
      "CPU times: total: 766 ms\n",
      "Wall time: 2.31 s\n",
      "CPU times: total: 875 ms\n",
      "Wall time: 2.51 s\n",
      "CPU times: total: 875 ms\n",
      "Wall time: 2.73 s\n",
      "14\n",
      "CPU times: total: 15.6 ms\n",
      "Wall time: 904 ms\n",
      "CPU times: total: 93.8 ms\n",
      "Wall time: 1.23 s\n",
      "CPU times: total: 250 ms\n",
      "Wall time: 1.2 s\n",
      "CPU times: total: 344 ms\n",
      "Wall time: 1.39 s\n",
      "CPU times: total: 438 ms\n",
      "Wall time: 1.96 s\n",
      "CPU times: total: 547 ms\n",
      "Wall time: 1.89 s\n",
      "CPU times: total: 766 ms\n",
      "Wall time: 2.36 s\n",
      "CPU times: total: 938 ms\n",
      "Wall time: 2.75 s\n",
      "CPU times: total: 1.03 s\n",
      "Wall time: 3.04 s\n",
      "15\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 895 ms\n",
      "CPU times: total: 78.1 ms\n",
      "Wall time: 1.04 s\n",
      "CPU times: total: 297 ms\n",
      "Wall time: 1.27 s\n",
      "CPU times: total: 219 ms\n",
      "Wall time: 1.5 s\n",
      "CPU times: total: 375 ms\n",
      "Wall time: 2.12 s\n",
      "CPU times: total: 828 ms\n",
      "Wall time: 2.56 s\n",
      "CPU times: total: 828 ms\n",
      "Wall time: 2.48 s\n",
      "CPU times: total: 1.09 s\n",
      "Wall time: 2.97 s\n",
      "CPU times: total: 1.08 s\n",
      "Wall time: 3.23 s\n",
      "16\n",
      "CPU times: total: 31.2 ms\n",
      "Wall time: 825 ms\n",
      "CPU times: total: 31.2 ms\n",
      "Wall time: 1.1 s\n",
      "CPU times: total: 203 ms\n",
      "Wall time: 1.43 s\n",
      "CPU times: total: 312 ms\n",
      "Wall time: 1.62 s\n",
      "CPU times: total: 688 ms\n",
      "Wall time: 2.02 s\n",
      "CPU times: total: 703 ms\n",
      "Wall time: 2.12 s\n",
      "CPU times: total: 1.03 s\n",
      "Wall time: 2.79 s\n",
      "CPU times: total: 922 ms\n",
      "Wall time: 2.5 s\n",
      "CPU times: total: 1.03 s\n",
      "Wall time: 3.27 s\n",
      "17\n",
      "CPU times: total: 15.6 ms\n",
      "Wall time: 941 ms\n",
      "CPU times: total: 172 ms\n",
      "Wall time: 1.19 s\n",
      "CPU times: total: 328 ms\n",
      "Wall time: 1.52 s\n",
      "CPU times: total: 453 ms\n",
      "Wall time: 1.63 s\n",
      "CPU times: total: 672 ms\n",
      "Wall time: 1.96 s\n",
      "CPU times: total: 734 ms\n",
      "Wall time: 2.21 s\n",
      "CPU times: total: 875 ms\n",
      "Wall time: 2.48 s\n",
      "CPU times: total: 766 ms\n",
      "Wall time: 2.75 s\n",
      "CPU times: total: 1.23 s\n",
      "Wall time: 3.3 s\n",
      "18\n",
      "CPU times: total: 15.6 ms\n",
      "Wall time: 1.03 s\n",
      "CPU times: total: 469 ms\n",
      "Wall time: 1.17 s\n",
      "CPU times: total: 375 ms\n",
      "Wall time: 1.54 s\n",
      "CPU times: total: 359 ms\n",
      "Wall time: 1.69 s\n",
      "CPU times: total: 500 ms\n",
      "Wall time: 1.86 s\n",
      "CPU times: total: 906 ms\n",
      "Wall time: 2.61 s\n",
      "CPU times: total: 766 ms\n",
      "Wall time: 2.54 s\n",
      "CPU times: total: 1.08 s\n",
      "Wall time: 3.05 s\n",
      "CPU times: total: 1.11 s\n",
      "Wall time: 2.78 s\n",
      "19\n",
      "CPU times: total: 15.6 ms\n",
      "Wall time: 969 ms\n",
      "CPU times: total: 312 ms\n",
      "Wall time: 1.22 s\n",
      "CPU times: total: 266 ms\n",
      "Wall time: 1.33 s\n",
      "CPU times: total: 250 ms\n",
      "Wall time: 1.5 s\n",
      "CPU times: total: 734 ms\n",
      "Wall time: 2.02 s\n",
      "CPU times: total: 781 ms\n",
      "Wall time: 2.05 s\n",
      "CPU times: total: 1.05 s\n",
      "Wall time: 2.76 s\n",
      "CPU times: total: 1.11 s\n",
      "Wall time: 3.07 s\n",
      "CPU times: total: 1.5 s\n",
      "Wall time: 3.66 s\n",
      "20\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 905 ms\n",
      "CPU times: total: 172 ms\n",
      "Wall time: 1.12 s\n",
      "CPU times: total: 203 ms\n",
      "Wall time: 1.37 s\n",
      "CPU times: total: 391 ms\n",
      "Wall time: 1.53 s\n",
      "CPU times: total: 688 ms\n",
      "Wall time: 2.07 s\n",
      "CPU times: total: 812 ms\n",
      "Wall time: 2.23 s\n",
      "CPU times: total: 1.05 s\n",
      "Wall time: 2.78 s\n",
      "CPU times: total: 1.08 s\n",
      "Wall time: 2.89 s\n",
      "CPU times: total: 1.17 s\n",
      "Wall time: 3.14 s\n",
      "21\n",
      "CPU times: total: 93.8 ms\n",
      "Wall time: 960 ms\n",
      "CPU times: total: 266 ms\n",
      "Wall time: 1.12 s\n",
      "CPU times: total: 266 ms\n",
      "Wall time: 1.36 s\n",
      "CPU times: total: 391 ms\n",
      "Wall time: 1.5 s\n",
      "CPU times: total: 844 ms\n",
      "Wall time: 2.21 s\n",
      "CPU times: total: 1.05 s\n",
      "Wall time: 2.56 s\n",
      "CPU times: total: 922 ms\n",
      "Wall time: 2.44 s\n",
      "CPU times: total: 984 ms\n",
      "Wall time: 2.64 s\n",
      "CPU times: total: 1.16 s\n",
      "Wall time: 3.48 s\n",
      "22\n",
      "CPU times: total: 31.2 ms\n",
      "Wall time: 1.02 s\n",
      "CPU times: total: 250 ms\n",
      "Wall time: 1.24 s\n",
      "CPU times: total: 250 ms\n",
      "Wall time: 1.28 s\n",
      "CPU times: total: 484 ms\n",
      "Wall time: 1.57 s\n",
      "CPU times: total: 828 ms\n",
      "Wall time: 2.18 s\n",
      "CPU times: total: 766 ms\n",
      "Wall time: 2.21 s\n",
      "CPU times: total: 1.23 s\n",
      "Wall time: 3.05 s\n",
      "CPU times: total: 1.02 s\n",
      "Wall time: 3.19 s\n",
      "CPU times: total: 1.38 s\n",
      "Wall time: 3.47 s\n",
      "23\n",
      "CPU times: total: 31.2 ms\n",
      "Wall time: 1.02 s\n",
      "CPU times: total: 406 ms\n",
      "Wall time: 1.18 s\n",
      "CPU times: total: 250 ms\n",
      "Wall time: 1.51 s\n",
      "CPU times: total: 406 ms\n",
      "Wall time: 1.65 s\n",
      "CPU times: total: 641 ms\n",
      "Wall time: 1.83 s\n",
      "CPU times: total: 875 ms\n",
      "Wall time: 2.23 s\n",
      "CPU times: total: 1.05 s\n",
      "Wall time: 2.78 s\n",
      "CPU times: total: 1.86 s\n",
      "Wall time: 3.97 s\n",
      "CPU times: total: 1.47 s\n",
      "Wall time: 4.03 s\n",
      "24\n",
      "CPU times: total: 15.6 ms\n",
      "Wall time: 950 ms\n",
      "CPU times: total: 391 ms\n",
      "Wall time: 1.21 s\n",
      "CPU times: total: 312 ms\n",
      "Wall time: 1.54 s\n",
      "CPU times: total: 422 ms\n",
      "Wall time: 1.47 s\n",
      "CPU times: total: 797 ms\n",
      "Wall time: 2.16 s\n",
      "CPU times: total: 1.17 s\n",
      "Wall time: 2.62 s\n",
      "CPU times: total: 1 s\n",
      "Wall time: 2.42 s\n",
      "CPU times: total: 1.55 s\n",
      "Wall time: 3.7 s\n",
      "CPU times: total: 1.78 s\n",
      "Wall time: 4.4 s\n",
      "25\n",
      "CPU times: total: 31.2 ms\n",
      "Wall time: 982 ms\n",
      "CPU times: total: 344 ms\n",
      "Wall time: 1.19 s\n",
      "CPU times: total: 406 ms\n",
      "Wall time: 1.53 s\n",
      "CPU times: total: 844 ms\n",
      "Wall time: 1.9 s\n",
      "CPU times: total: 891 ms\n",
      "Wall time: 2.08 s\n",
      "CPU times: total: 1.36 s\n",
      "Wall time: 2.95 s\n",
      "CPU times: total: 1.75 s\n",
      "Wall time: 3.58 s\n",
      "CPU times: total: 1.38 s\n",
      "Wall time: 3.17 s\n",
      "CPU times: total: 1.91 s\n",
      "Wall time: 4.04 s\n",
      "26\n",
      "CPU times: total: 46.9 ms\n",
      "Wall time: 954 ms\n",
      "CPU times: total: 344 ms\n",
      "Wall time: 1.13 s\n",
      "CPU times: total: 656 ms\n",
      "Wall time: 1.56 s\n",
      "CPU times: total: 750 ms\n",
      "Wall time: 1.61 s\n",
      "CPU times: total: 938 ms\n",
      "Wall time: 2.34 s\n",
      "CPU times: total: 1.58 s\n",
      "Wall time: 3.09 s\n",
      "CPU times: total: 1.25 s\n",
      "Wall time: 3.3 s\n",
      "CPU times: total: 1.53 s\n",
      "Wall time: 3.2 s\n",
      "CPU times: total: 1.86 s\n",
      "Wall time: 3.8 s\n",
      "27\n",
      "CPU times: total: 46.9 ms\n",
      "Wall time: 1.02 s\n",
      "CPU times: total: 344 ms\n",
      "Wall time: 1.22 s\n",
      "CPU times: total: 562 ms\n",
      "Wall time: 1.76 s\n",
      "CPU times: total: 703 ms\n",
      "Wall time: 1.76 s\n",
      "CPU times: total: 1.3 s\n",
      "Wall time: 2.38 s\n",
      "CPU times: total: 1.17 s\n",
      "Wall time: 2.85 s\n",
      "CPU times: total: 1.45 s\n",
      "Wall time: 3.36 s\n",
      "CPU times: total: 1.39 s\n",
      "Wall time: 3.24 s\n",
      "CPU times: total: 2.02 s\n",
      "Wall time: 3.83 s\n",
      "28\n",
      "CPU times: total: 93.8 ms\n",
      "Wall time: 1.02 s\n",
      "CPU times: total: 281 ms\n",
      "Wall time: 1.36 s\n",
      "CPU times: total: 594 ms\n",
      "Wall time: 1.81 s\n",
      "CPU times: total: 766 ms\n",
      "Wall time: 1.81 s\n",
      "CPU times: total: 1.2 s\n",
      "Wall time: 2.44 s\n",
      "CPU times: total: 1.2 s\n",
      "Wall time: 2.73 s\n",
      "CPU times: total: 1.72 s\n",
      "Wall time: 3.42 s\n",
      "CPU times: total: 1.52 s\n",
      "Wall time: 3.27 s\n",
      "CPU times: total: 1.86 s\n",
      "Wall time: 3.9 s\n",
      "29\n",
      "CPU times: total: 78.1 ms\n",
      "Wall time: 1.1 s\n",
      "CPU times: total: 250 ms\n",
      "Wall time: 1.47 s\n",
      "CPU times: total: 438 ms\n",
      "Wall time: 1.7 s\n",
      "CPU times: total: 922 ms\n",
      "Wall time: 1.82 s\n",
      "CPU times: total: 1.22 s\n",
      "Wall time: 2.65 s\n",
      "CPU times: total: 1.31 s\n",
      "Wall time: 2.73 s\n",
      "CPU times: total: 1.61 s\n",
      "Wall time: 3.44 s\n",
      "CPU times: total: 1.55 s\n",
      "Wall time: 3.56 s\n",
      "CPU times: total: 1.78 s\n",
      "Wall time: 3.91 s\n",
      "30\n",
      "CPU times: total: 15.6 ms\n",
      "Wall time: 1.12 s\n",
      "CPU times: total: 266 ms\n",
      "Wall time: 1.48 s\n",
      "CPU times: total: 422 ms\n",
      "Wall time: 1.45 s\n",
      "CPU times: total: 859 ms\n",
      "Wall time: 1.7 s\n",
      "CPU times: total: 1.23 s\n",
      "Wall time: 2.48 s\n",
      "CPU times: total: 1.48 s\n",
      "Wall time: 3.4 s\n",
      "CPU times: total: 1.42 s\n",
      "Wall time: 3.26 s\n",
      "CPU times: total: 1.78 s\n",
      "Wall time: 3.91 s\n",
      "CPU times: total: 1.81 s\n",
      "Wall time: 4.02 s\n",
      "31\n",
      "CPU times: total: 109 ms\n",
      "Wall time: 1.01 s\n",
      "CPU times: total: 250 ms\n",
      "Wall time: 1.5 s\n",
      "CPU times: total: 328 ms\n",
      "Wall time: 1.43 s\n",
      "CPU times: total: 875 ms\n",
      "Wall time: 2.08 s\n",
      "CPU times: total: 1.44 s\n",
      "Wall time: 2.58 s\n",
      "CPU times: total: 1.22 s\n",
      "Wall time: 2.75 s\n",
      "CPU times: total: 1.92 s\n",
      "Wall time: 3.78 s\n",
      "CPU times: total: 1.56 s\n",
      "Wall time: 3.59 s\n",
      "CPU times: total: 2.16 s\n",
      "Wall time: 4.68 s\n",
      "32\n",
      "CPU times: total: 78.1 ms\n",
      "Wall time: 1.05 s\n",
      "CPU times: total: 250 ms\n",
      "Wall time: 1.36 s\n",
      "CPU times: total: 500 ms\n",
      "Wall time: 1.65 s\n",
      "CPU times: total: 906 ms\n",
      "Wall time: 2 s\n",
      "CPU times: total: 1.25 s\n",
      "Wall time: 2.42 s\n",
      "CPU times: total: 1.5 s\n",
      "Wall time: 2.79 s\n",
      "CPU times: total: 1.64 s\n",
      "Wall time: 3.11 s\n",
      "CPU times: total: 1.89 s\n",
      "Wall time: 3.86 s\n",
      "CPU times: total: 2.03 s\n",
      "Wall time: 4.09 s\n"
     ]
    }
   ],
   "source": [
    "iteration = 0\n",
    "while iteration < 100:\n",
    "  print(iteration)\n",
    "  for size in sample_sizes:\n",
    "    random_indices = random.sample(range(len(base_sents)), size)\n",
    "    sents = []\n",
    "    tags = []\n",
    "    for index in random_indices:\n",
    "      sents.append(base_sents[index])\n",
    "      tags.append(base_tags[index])\n",
    "    for idx, sent in enumerate(sents):\n",
    "      sent.insert(0, 'start')\n",
    "      sent.append('stop')\n",
    "      sents[idx] = sent\n",
    "    for idx, sent in enumerate(tags):\n",
    "      sent.insert(0, 'start')\n",
    "      sent.append('stop')\n",
    "      tags[idx] = sent\n",
    "    rawwords = []\n",
    "    for sent in sents:\n",
    "      for word in sent:\n",
    "        rawwords.append(word)\n",
    "\n",
    "    rawtags = []\n",
    "    for sequence in tags:\n",
    "      for tag in sequence:\n",
    "        rawtags.append(tag)\n",
    "\n",
    "    allwords = list(set(rawwords))\n",
    "    alltags = list(set(rawtags))\n",
    "\n",
    "    word_tokenizer = {word: idx+1 for idx, word in enumerate(allwords)}\n",
    "    word_decoder = {idx+1: word for idx, word in enumerate(allwords)}\n",
    "    tag_tokenizer = {tag: idx+1 for idx, tag in enumerate(alltags)}\n",
    "    tag_decoder = {idx+1: tag for idx, tag in enumerate(alltags)}\n",
    "\n",
    "    def tokenize(sentences, tokenizer):\n",
    "      indexed_sentences = []\n",
    "      for sentence in sentences:\n",
    "        indexed_sentence = [tokenizer[word] for word in sentence]\n",
    "        indexed_sentences.append(indexed_sentence)\n",
    "      return indexed_sentences\n",
    "\n",
    "    encsents = tokenize(sents, word_tokenizer)\n",
    "    enctags = tokenize(tags, tag_tokenizer)\n",
    "\n",
    "    padsents, padtags = [], []\n",
    "\n",
    "    maxlen = max(len(sublist) for sublist in encsents)\n",
    "    for sublist in encsents:\n",
    "      while len(sublist) < maxlen:\n",
    "        sublist = sublist + [0]\n",
    "        if len(sublist) == maxlen:\n",
    "          break\n",
    "        sublist = [0] + sublist\n",
    "      padsents.append(sublist)\n",
    "    allenctags = []\n",
    "    maxlen = max(len(sublist) for sublist in enctags)\n",
    "    for sublist in enctags:\n",
    "      for i in sublist:\n",
    "        allenctags.append(i)\n",
    "      while len(sublist) < maxlen:\n",
    "        sublist = sublist + [0]\n",
    "        if len(sublist) == maxlen:\n",
    "          break\n",
    "        sublist = [0] + sublist\n",
    "      padtags.append(sublist)\n",
    "\n",
    "    tag_mask = []\n",
    "    for seq in padtags:\n",
    "      mask = [1]*len(seq)\n",
    "      for idx, tag in enumerate(seq):\n",
    "        if tag == tag_tokenizer['XX']:\n",
    "          mask[idx] = 0\n",
    "        if tag == tag_tokenizer['start']:\n",
    "          mask[idx] = 0\n",
    "        if tag == tag_tokenizer['stop']:\n",
    "          mask[idx] = 0\n",
    "        if tag == tag_tokenizer['PUNCT']:\n",
    "          mask[idx] = 0\n",
    "      tag_mask.append(mask)\n",
    "    for i in range(len(tag_mask)):\n",
    "      if len(tag_mask[i]) != len(padsents[i]):\n",
    "        print(i)\n",
    "    pad_tags_mask = []\n",
    "    for i in range(len(padtags)):\n",
    "      pad_tags_mask.append([padtags[i], tag_mask[i]])\n",
    "    X_test, X_train, y_test, y_train = train_test_split(padsents, pad_tags_mask, test_size=0.8)\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.long).to(device)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.long).to(device)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "    batch_size = 64\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    class_weights = calculate_class_weights(allenctags)\n",
    "\n",
    "    class Model(nn.Module):\n",
    "      def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "      def categorical_accuracy(self, preds, y, mask, tag_pad_idx = 0):\n",
    "        max_preds = preds.argmax(dim = 1, keepdim = False)\n",
    "        max_preds = max_preds*mask\n",
    "        y = y*mask\n",
    "        max_preds = torch.flatten(max_preds)\n",
    "        y = torch.flatten(y)\n",
    "        non_pad_elements = y.nonzero()\n",
    "        correct = max_preds[non_pad_elements].eq(y[non_pad_elements])\n",
    "        return correct.sum() / y[non_pad_elements].shape[0]\n",
    "\n",
    "      def early_stop(self, validation_loss, patience = 3, min_delta = 0):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.patiencecount = 0\n",
    "        elif validation_loss > (self.min_validation_loss + min_delta):\n",
    "            self.patiencecount += 1\n",
    "            #print(f\"Early stopping counter: {self.patiencecount} out of {patience}\")\n",
    "            if self.patiencecount >= patience:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "      def fit(self, train_dl, val_dl, epochs, pad_idx = 0):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr = 0.001)\n",
    "\n",
    "        self.patiencecount = 0\n",
    "        self.min_validation_loss = float('inf')\n",
    "\n",
    "        self.trainlosses = []\n",
    "        self.vallosses = []\n",
    "\n",
    "        self.train_accs = []\n",
    "        self.val_accs = []\n",
    "\n",
    "        counter = 0\n",
    "        for epoch in range(epochs):\n",
    "          for batch in train_dl:\n",
    "            counter += 1\n",
    "            text = batch[0]\n",
    "\n",
    "            output = batch[1]\n",
    "            output = output.transpose(0,1)\n",
    "            tags = output[0]\n",
    "            mask = output[1]\n",
    "            optimizer.zero_grad()\n",
    "            predictions = self.forward(text)\n",
    "            predictions = predictions.transpose(1,2)\n",
    "            loss = self.loss_fn(predictions, tags)\n",
    "            loss = (loss * mask).sum() / mask.sum()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "          train_loss, train_acc = self.evaluate(train_dl)\n",
    "          val_loss, val_acc = self.evaluate(val_dl)\n",
    "          self.trainlosses.append(train_loss)\n",
    "          self.vallosses.append(val_loss)\n",
    "          self.train_accs.append(train_acc)\n",
    "          self.val_accs.append(val_acc)\n",
    "          if self.early_stop(val_loss):\n",
    "            break\n",
    "          train_acc = train_acc*100\n",
    "          val_acc = val_acc*100\n",
    "          #print(f\"Epoch [{epoch + 1}/{epochs}] - TrainLoss: {train_loss:.4f}, ValLoss: {val_loss:.4f}, TrainAcc: {train_acc:.2f},% ValAcc: {val_acc:.2f}%\")\n",
    "\n",
    "      def evaluate(self, val_dl, pad_idx = 0):\n",
    "        losses = []\n",
    "        accuracies = []\n",
    "        with torch.no_grad():\n",
    "          for batch in val_dl:\n",
    "              text = batch[0]\n",
    "              output = batch[1]\n",
    "              output = output.transpose(0,1)\n",
    "              tags = output[0]\n",
    "              mask = output[1]\n",
    "              predictions = self.forward(text)\n",
    "              predictions = predictions.transpose(1,2)\n",
    "              loss = self.loss_fn(predictions, tags)\n",
    "              loss = (loss * mask).sum() / mask.sum()\n",
    "              losses.append(loss)\n",
    "              acc = self.categorical_accuracy(predictions, tags, mask, pad_idx)\n",
    "              accuracies.append(acc)\n",
    "        return torch.Tensor(losses).mean(), torch.Tensor(accuracies).mean()\n",
    "\n",
    "\n",
    "    class OSTagger(Model):\n",
    "        def __init__(self, vocab, embeds, hidden, tagset, n_layers, dropout, criterion, pad_idx = 0):\n",
    "            super().__init__()\n",
    "\n",
    "            self.embedding = nn.Embedding(vocab, embeds, padding_idx = pad_idx, scale_grad_by_freq = True)\n",
    "\n",
    "            self.lstm = nn.LSTM(embeds, hidden, num_layers = n_layers, bidirectional = True,\n",
    "                                dropout = dropout if n_layers > 1 else 0)\n",
    "\n",
    "            self.lin = nn.Linear(hidden * 2, tagset)\n",
    "\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "            self.loss_fn = criterion\n",
    "\n",
    "        def forward(self, sent):\n",
    "            x = self.embedding(sent)\n",
    "            x, (hidden, cell) = self.lstm(x)\n",
    "            x = self.lin(self.dropout(x))\n",
    "            return x\n",
    "\n",
    "    vocab = len(allwords)+1\n",
    "    n_emb = 300\n",
    "    n_hidden = 60\n",
    "    n_tags = len(alltags)+1\n",
    "    n_layers = 3\n",
    "    dropout = .2\n",
    "    pad_idx = 0\n",
    "    criterion = nn.CrossEntropyLoss(reduction = 'none', weight = class_weights, ignore_index = pad_idx).to(device)\n",
    "\n",
    "    network_OSTagger = OSTagger(vocab, n_emb, n_hidden, n_tags, n_layers, dropout, criterion, pad_idx).to(device)\n",
    "\n",
    "    %time network_OSTagger.fit(train_loader, test_loader, 100)\n",
    "\n",
    "    preds, tags, texts = predict(network_OSTagger, test_loader, pad_idx)\n",
    "\n",
    "    for i in range(len(preds)):\n",
    "      for j in range(len(preds[i])):\n",
    "        if tags[i][j] == 0:\n",
    "          preds[i][j] = 0\n",
    "      preds[i] = [ele for ele in preds[i] if ele != 0]\n",
    "      tags[i] = [ele for ele in tags[i] if ele != 0]\n",
    "      texts[i] = [ele for ele in texts[i] if ele != 0]\n",
    "    decpreds, dectags, dectext, declangs = [], [], [], []\n",
    "    decpreds.append(tokenize(preds, tag_decoder))\n",
    "    dectags.append(tokenize(tags, tag_decoder))\n",
    "    dectext.append(tokenize(texts, word_decoder))\n",
    "    decpreds = decpreds[0]\n",
    "    dectags = dectags[0]\n",
    "    dectext = dectext[0]\n",
    "\n",
    "\n",
    "    corrects, incorrects, totals = [], [], []\n",
    "    for i in range(len(decpreds)):\n",
    "      for j in range(len(decpreds[i])):\n",
    "        totals.append(dectags[i][j])\n",
    "        if decpreds[i][j] == dectags[i][j]:\n",
    "          corrects.append(decpreds[i][j])\n",
    "        else:\n",
    "          incorrects.append(decpreds[i][j])\n",
    "\n",
    "    c_freqs = Counter(corrects)\n",
    "    ic_freqs = Counter(incorrects)\n",
    "    t_freqs = Counter(totals)\n",
    "\n",
    "    data = []\n",
    "    labels = []\n",
    "    outs = []\n",
    "    any_c = [x[0] for x in c_freqs.most_common()]\n",
    "    for i in t_freqs.most_common():\n",
    "      if i[0] not in any_c:\n",
    "        data.append(0)\n",
    "        labels.append(i[0])\n",
    "        outs.append((i[0], 0))\n",
    "        results_df.loc[len(results_df)] = [i[0], 0, size]\n",
    "      else:\n",
    "        for ele in c_freqs.most_common():\n",
    "          if ele[0] == i[0]:\n",
    "            num_c = ele[1]\n",
    "        class_acc = num_c/i[1]\n",
    "        data.append(class_acc*100)\n",
    "        labels.append(i[0])\n",
    "        outs.append((i[0], class_acc*100))\n",
    "        results_df.loc[len(results_df)] = [i[0], class_acc*100, size]\n",
    "\n",
    "  results_df.to_csv('resultsOS.csv', index=False)\n",
    "  with torch.no_grad():\n",
    "      torch.cuda.empty_cache()\n",
    "  iteration = iteration + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1740715114732,
     "user": {
      "displayName": "Carter Smith",
      "userId": "16068926545987539689"
     },
     "user_tz": 360
    },
    "id": "Rwd9TQ0AfLjg"
   },
   "outputs": [],
   "source": [
    "results_df.to_csv('resultsOS.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNQDB2NxsZhbWAwSvDkoIxa",
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
