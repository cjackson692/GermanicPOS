{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":8106,"status":"ok","timestamp":1743010762947,"user":{"displayName":"Carter Smith","userId":"16068926545987539689"},"user_tz":300},"id":"JLWCPXV7bpnu"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import spacy\n","import numpy as np\n","import time\n","import random\n","from collections import Counter\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import TensorDataset, DataLoader\n","import matplotlib.pyplot as plt\n","import math\n","import pandas as pd"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"JOI-cjpxiAdu","executionInfo":{"status":"ok","timestamp":1743010763027,"user_tz":300,"elapsed":48,"user":{"displayName":"Carter Smith","userId":"16068926545987539689"}},"outputId":"3c733209-b611-44b2-b828-dd77a8830cd2"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'12.4'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":2}],"source":["torch.version.cuda"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29,"status":"ok","timestamp":1743010763058,"user":{"displayName":"Carter Smith","userId":"16068926545987539689"},"user_tz":300},"id":"mjKmP6NjbrVv","outputId":"87568342-103b-4699-c9bb-66b0b3466ec0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Script is running with GPU\n"]}],"source":["if torch.cuda.is_available():\n","  device = torch.device(\"cuda\")\n","  print('Script is running with GPU')\n","else:\n","  device = torch.device(\"cpu\")\n","  print('Script is running WITHOUT GPU')"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8DBAewO8iAdz","executionInfo":{"status":"ok","timestamp":1743010763078,"user_tz":300,"elapsed":17,"user":{"displayName":"Carter Smith","userId":"16068926545987539689"}},"outputId":"5bedd466-b511-4911-ada8-f2bc12b39069"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":4}],"source":["torch.cuda.is_available()\n"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1743010763083,"user":{"displayName":"Carter Smith","userId":"16068926545987539689"},"user_tz":300},"id":"gxTpg9Zrdtmh"},"outputs":[],"source":["def calculate_class_weights(y):\n","    unique_classes, class_counts = np.unique(y, return_counts=True)\n","    total_samples = len(y)\n","    class_weights = []\n","    class_weights.append(1)\n","\n","    for class_label, class_count in zip(unique_classes, class_counts):\n","        class_weight = math.log(total_samples / (class_count))\n","        class_weights.append(class_weight)\n","\n","    class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n","\n","    return class_weights"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1743010763091,"user":{"displayName":"Carter Smith","userId":"16068926545987539689"},"user_tz":300},"id":"OFG0VTkBeHRO"},"outputs":[],"source":["def predict(model, data, pad_idx):\n","    with torch.no_grad():\n","      predicts_out = []\n","      tags_out = []\n","      text_out = []\n","      for batch in data:\n","        text = batch[0]\n","        output = batch[1]\n","        output = output.transpose(0,1)\n","        tags = output[0]\n","        mask = output[1]\n","        predictions = model.forward(text)\n","        predictions = predictions.transpose(1,2)\n","        predictions = predictions.argmax(dim = 1)\n","        for idx, sent in enumerate(text):\n","          predicts_out.append(np.asarray(predictions[idx].cpu()))\n","          tags_out.append(np.asarray(tags[idx].cpu()))\n","          text_out.append(np.asarray(sent.cpu()))\n","    for idx, ele in enumerate(predicts_out):\n","      predicts_out[idx] = predicts_out[idx].tolist()\n","    for idx, ele in enumerate(tags_out):\n","      tags_out[idx] = tags_out[idx].tolist()\n","    for idx, ele in enumerate(text_out):\n","      text_out[idx] = text_out[idx].tolist()\n","\n","    return predicts_out, tags_out, text_out"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":19,"status":"error","timestamp":1743010777636,"user":{"displayName":"Carter Smith","userId":"16068926545987539689"},"user_tz":300},"id":"Dzjjb9j_b61c","colab":{"base_uri":"https://localhost:8080/","height":314},"outputId":"7d6bf093-d16a-4d46-9448-95af198455a0"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'OHG_sents_test.npy'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-0f3bcc5abd83>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbase_sents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'OHG_sents_test.npy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mbase_tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'OHG_tags_minimal_test.npy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mproblems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/lib/_npyio_impl.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    453\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'OHG_sents_test.npy'"]}],"source":["base_sents = list(np.load('OHG_sents_test.npy', allow_pickle=True))\n","base_tags = list(np.load('OHG_tags_minimal_test.npy', allow_pickle=True))\n","problems = []"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8607,"status":"aborted","timestamp":1743010763220,"user":{"displayName":"Carter Smith","userId":"16068926545987539689"},"user_tz":300},"id":"GOB7uA0dyO2R"},"outputs":[],"source":["nums = []\n","for sent_idx, sent in enumerate(base_sents):\n","    for word_idx, tag in enumerate(base_tags[sent_idx]):\n","        if tag == 'NUM':\n","          nums.append((base_sents[sent_idx][word_idx], base_tags[sent_idx][word_idx]))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8609,"status":"aborted","timestamp":1743010763223,"user":{"displayName":"Carter Smith","userId":"16068926545987539689"},"user_tz":300},"id":"7lgmG0GzzG2j"},"outputs":[],"source":["from collections import Counter\n","num_counts = Counter(nums)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8648,"status":"aborted","timestamp":1743010763266,"user":{"displayName":"Carter Smith","userId":"16068926545987539689"},"user_tz":300},"id":"WNFxUu9bcJzu"},"outputs":[],"source":["len(base_sents)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8663,"status":"aborted","timestamp":1743010763281,"user":{"displayName":"Carter Smith","userId":"16068926545987539689"},"user_tz":300},"id":"hqD-nAXPb3mb"},"outputs":[],"source":["sample_sizes = [250, 500, 750, 1000, 1500, 1683]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8773,"status":"aborted","timestamp":1743010763392,"user":{"displayName":"Carter Smith","userId":"16068926545987539689"},"user_tz":300},"id":"p7XDSAldcfdy"},"outputs":[],"source":["results_df = pd.DataFrame(columns=[\"tag\", \"accuracy\", \"sample_size\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LT4QGOvzy-RO"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pFUmj4WPiAeD"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"2ab1COwJbug4","outputId":"3ad37e16-556e-47cd-a38a-e68d20e23dc5"},"outputs":[{"name":"stdout","output_type":"stream","text":["0\n","CPU times: total: 2.08 s\n","Wall time: 2.92 s\n","CPU times: total: 953 ms\n","Wall time: 1 s\n","CPU times: total: 1.09 s\n","Wall time: 1.17 s\n","CPU times: total: 1.11 s\n","Wall time: 1.2 s\n","CPU times: total: 1.38 s\n","Wall time: 1.52 s\n","CPU times: total: 1.64 s\n","Wall time: 1.73 s\n","1\n","CPU times: total: 734 ms\n","Wall time: 754 ms\n","CPU times: total: 969 ms\n","Wall time: 1.1 s\n","CPU times: total: 1.09 s\n","Wall time: 1.21 s\n","CPU times: total: 1.28 s\n","Wall time: 1.33 s\n","CPU times: total: 1.25 s\n","Wall time: 1.3 s\n","CPU times: total: 1.59 s\n","Wall time: 1.63 s\n","2\n","CPU times: total: 656 ms\n","Wall time: 706 ms\n","CPU times: total: 953 ms\n","Wall time: 970 ms\n","CPU times: total: 1.17 s\n","Wall time: 1.3 s\n","CPU times: total: 1.23 s\n","Wall time: 1.27 s\n","CPU times: total: 1.66 s\n","Wall time: 1.76 s\n","CPU times: total: 1.58 s\n","Wall time: 1.69 s\n","3\n","CPU times: total: 734 ms\n","Wall time: 758 ms\n","CPU times: total: 922 ms\n","Wall time: 914 ms\n","CPU times: total: 1.08 s\n","Wall time: 1.15 s\n","CPU times: total: 1.23 s\n","Wall time: 1.3 s\n","CPU times: total: 1.44 s\n","Wall time: 1.6 s\n","CPU times: total: 1.69 s\n","Wall time: 1.88 s\n","4\n","CPU times: total: 797 ms\n","Wall time: 871 ms\n","CPU times: total: 812 ms\n","Wall time: 922 ms\n","CPU times: total: 969 ms\n","Wall time: 1.11 s\n","CPU times: total: 1.27 s\n","Wall time: 1.28 s\n","CPU times: total: 1.52 s\n","Wall time: 1.61 s\n","CPU times: total: 1.86 s\n","Wall time: 2.06 s\n","5\n","CPU times: total: 828 ms\n","Wall time: 884 ms\n","CPU times: total: 1.2 s\n","Wall time: 1.32 s\n","CPU times: total: 1.31 s\n","Wall time: 1.42 s\n","CPU times: total: 1.23 s\n","Wall time: 1.31 s\n","CPU times: total: 1.42 s\n","Wall time: 1.6 s\n","CPU times: total: 2 s\n","Wall time: 2.16 s\n","6\n","CPU times: total: 828 ms\n","Wall time: 896 ms\n","CPU times: total: 953 ms\n","Wall time: 1.1 s\n","CPU times: total: 1.03 s\n","Wall time: 1.28 s\n","CPU times: total: 1.22 s\n","Wall time: 1.36 s\n","CPU times: total: 1.45 s\n","Wall time: 1.52 s\n","CPU times: total: 1.81 s\n","Wall time: 1.86 s\n","7\n","CPU times: total: 797 ms\n","Wall time: 808 ms\n","CPU times: total: 1.02 s\n","Wall time: 1.07 s\n","CPU times: total: 984 ms\n","Wall time: 1.05 s\n","CPU times: total: 1.45 s\n","Wall time: 1.55 s\n","CPU times: total: 1.59 s\n","Wall time: 1.67 s\n","CPU times: total: 1.66 s\n","Wall time: 1.78 s\n","8\n","CPU times: total: 844 ms\n","Wall time: 877 ms\n","CPU times: total: 953 ms\n","Wall time: 1.04 s\n","CPU times: total: 1.09 s\n","Wall time: 1.16 s\n","CPU times: total: 1.11 s\n","Wall time: 1.26 s\n","CPU times: total: 1.36 s\n","Wall time: 1.62 s\n","CPU times: total: 1.81 s\n","Wall time: 2.03 s\n","9\n","CPU times: total: 891 ms\n","Wall time: 937 ms\n","CPU times: total: 1.08 s\n","Wall time: 1.09 s\n","CPU times: total: 1.58 s\n","Wall time: 1.68 s\n","CPU times: total: 1.44 s\n","Wall time: 1.53 s\n","CPU times: total: 1.47 s\n","Wall time: 1.75 s\n","CPU times: total: 1.84 s\n","Wall time: 2.14 s\n","10\n","CPU times: total: 828 ms\n","Wall time: 848 ms\n","CPU times: total: 1.05 s\n","Wall time: 1.07 s\n","CPU times: total: 1.14 s\n","Wall time: 1.24 s\n","CPU times: total: 1.59 s\n","Wall time: 1.7 s\n","CPU times: total: 1.89 s\n","Wall time: 2.08 s\n","CPU times: total: 2 s\n","Wall time: 2.21 s\n","11\n","CPU times: total: 844 ms\n","Wall time: 861 ms\n","CPU times: total: 1.16 s\n","Wall time: 1.24 s\n","CPU times: total: 1.19 s\n","Wall time: 1.36 s\n","CPU times: total: 1.33 s\n","Wall time: 1.48 s\n","CPU times: total: 1.42 s\n","Wall time: 1.68 s\n","CPU times: total: 1.92 s\n","Wall time: 2.06 s\n","12\n","CPU times: total: 797 ms\n","Wall time: 811 ms\n","CPU times: total: 1.03 s\n","Wall time: 1.06 s\n","CPU times: total: 1.53 s\n","Wall time: 1.71 s\n","CPU times: total: 1.59 s\n","Wall time: 1.76 s\n","CPU times: total: 2.12 s\n","Wall time: 2.4 s\n","CPU times: total: 1.81 s\n","Wall time: 1.92 s\n","13\n","CPU times: total: 859 ms\n","Wall time: 852 ms\n","CPU times: total: 1.14 s\n","Wall time: 1.17 s\n","CPU times: total: 1.36 s\n","Wall time: 1.37 s\n","CPU times: total: 1.42 s\n","Wall time: 1.47 s\n","CPU times: total: 2 s\n","Wall time: 2.04 s\n","CPU times: total: 1.88 s\n","Wall time: 2.07 s\n","14\n","CPU times: total: 953 ms\n","Wall time: 995 ms\n","CPU times: total: 1.09 s\n","Wall time: 1.13 s\n","CPU times: total: 1.22 s\n","Wall time: 1.33 s\n","CPU times: total: 1.45 s\n","Wall time: 1.51 s\n","CPU times: total: 1.89 s\n","Wall time: 1.96 s\n","CPU times: total: 2.03 s\n","Wall time: 2.03 s\n"]}],"source":["iteration = 0\n","while iteration < 25:\n","  print(iteration)\n","  for size in sample_sizes:\n","    random_indices = random.sample(range(len(base_sents)), size)\n","    sents = []\n","    tags = []\n","    for index in random_indices:\n","      sents.append(base_sents[index])\n","      tags.append(base_tags[index])\n","    for idx, sent in enumerate(sents):\n","      sent.insert(0, 'start')\n","      sent.append('stop')\n","      sents[idx] = sent\n","    for idx, sent in enumerate(tags):\n","      sent.insert(0, 'start')\n","      sent.append('stop')\n","      tags[idx] = sent\n","    rawwords = []\n","    for sent in sents:\n","      for word in sent:\n","        rawwords.append(word)\n","\n","    rawtags = []\n","    for sequence in tags:\n","      for tag in sequence:\n","        rawtags.append(tag)\n","\n","    allwords = list(set(rawwords))\n","    alltags = list(set(rawtags))\n","\n","    word_tokenizer = {word: idx+1 for idx, word in enumerate(allwords)}\n","    word_decoder = {idx+1: word for idx, word in enumerate(allwords)}\n","    tag_tokenizer = {tag: idx+1 for idx, tag in enumerate(alltags)}\n","    tag_decoder = {idx+1: tag for idx, tag in enumerate(alltags)}\n","\n","    def tokenize(sentences, tokenizer):\n","      indexed_sentences = []\n","      for sentence in sentences:\n","        indexed_sentence = [tokenizer[word] for word in sentence]\n","        indexed_sentences.append(indexed_sentence)\n","      return indexed_sentences\n","\n","    encsents = tokenize(sents, word_tokenizer)\n","    enctags = tokenize(tags, tag_tokenizer)\n","\n","    padsents, padtags = [], []\n","\n","    maxlen = max(len(sublist) for sublist in encsents)\n","    for sublist in encsents:\n","      while len(sublist) < maxlen:\n","        sublist = sublist + [0]\n","        if len(sublist) == maxlen:\n","          break\n","        sublist = [0] + sublist\n","      padsents.append(sublist)\n","    allenctags = []\n","    maxlen = max(len(sublist) for sublist in enctags)\n","    for sublist in enctags:\n","      for i in sublist:\n","        allenctags.append(i)\n","      while len(sublist) < maxlen:\n","        sublist = sublist + [0]\n","        if len(sublist) == maxlen:\n","          break\n","        sublist = [0] + sublist\n","      padtags.append(sublist)\n","\n","    tag_mask = []\n","    for seq in padtags:\n","      mask = [1]*len(seq)\n","      for idx, tag in enumerate(seq):\n","        if 'XX' in list(tag_tokenizer.keys()):\n","          if tag == tag_tokenizer['XX']:\n","            mask[idx] = 0\n","        if tag == tag_tokenizer['start']:\n","          mask[idx] = 0\n","        if tag == tag_tokenizer['stop']:\n","          mask[idx] = 0\n","        if tag == tag_tokenizer['PUNCT']:\n","          mask[idx] = 0\n","      tag_mask.append(mask)\n","    for i in range(len(tag_mask)):\n","      if len(tag_mask[i]) != len(padsents[i]):\n","        print(i)\n","    pad_tags_mask = []\n","    for i in range(len(padtags)):\n","      pad_tags_mask.append([padtags[i], tag_mask[i]])\n","    X_test, X_train, y_test, y_train = train_test_split(padsents, pad_tags_mask, test_size=0.8)\n","    X_train_tensor = torch.tensor(X_train, dtype=torch.long).to(device)\n","    y_train_tensor = torch.tensor(y_train, dtype=torch.long).to(device)\n","    X_test_tensor = torch.tensor(X_test, dtype=torch.long).to(device)\n","    y_test_tensor = torch.tensor(y_test, dtype=torch.long).to(device)\n","    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n","    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n","    batch_size = 64\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size)\n","    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n","\n","    class_weights = calculate_class_weights(allenctags)\n","\n","    class Model(nn.Module):\n","      def __init__(self):\n","        super().__init__()\n","\n","      def categorical_accuracy(self, preds, y, mask, tag_pad_idx = 0):\n","        max_preds = preds.argmax(dim = 1, keepdim = False)\n","        max_preds = max_preds*mask\n","        y = y*mask\n","        max_preds = torch.flatten(max_preds)\n","        y = torch.flatten(y)\n","        non_pad_elements = y.nonzero()\n","        correct = max_preds[non_pad_elements].eq(y[non_pad_elements])\n","        return correct.sum() / y[non_pad_elements].shape[0]\n","\n","      def early_stop(self, validation_loss, patience = 3, min_delta = 0):\n","        if validation_loss < self.min_validation_loss:\n","            self.min_validation_loss = validation_loss\n","            self.patiencecount = 0\n","        elif validation_loss > (self.min_validation_loss + min_delta):\n","            self.patiencecount += 1\n","            #print(f\"Early stopping counter: {self.patiencecount} out of {patience}\")\n","            if self.patiencecount >= patience:\n","                return True\n","        return False\n","\n","      def fit(self, train_dl, val_dl, epochs, pad_idx = 0):\n","        optimizer = torch.optim.Adam(self.parameters(), lr = 0.001)\n","\n","        self.patiencecount = 0\n","        self.min_validation_loss = float('inf')\n","\n","        self.trainlosses = []\n","        self.vallosses = []\n","\n","        self.train_accs = []\n","        self.val_accs = []\n","\n","        counter = 0\n","        for epoch in range(epochs):\n","          for batch in train_dl:\n","            counter += 1\n","            text = batch[0]\n","\n","            output = batch[1]\n","            output = output.transpose(0,1)\n","            tags = output[0]\n","            mask = output[1]\n","            optimizer.zero_grad()\n","            predictions = self.forward(text)\n","            predictions = predictions.transpose(1,2)\n","            loss = self.loss_fn(predictions, tags)\n","            loss = (loss * mask).sum() / mask.sum()\n","            loss.backward()\n","            optimizer.step()\n","          train_loss, train_acc = self.evaluate(train_dl)\n","          val_loss, val_acc = self.evaluate(val_dl)\n","          self.trainlosses.append(train_loss)\n","          self.vallosses.append(val_loss)\n","          self.train_accs.append(train_acc)\n","          self.val_accs.append(val_acc)\n","          if self.early_stop(val_loss):\n","            break\n","          train_acc = train_acc*100\n","          val_acc = val_acc*100\n","          #print(f\"Epoch [{epoch + 1}/{epochs}] - TrainLoss: {train_loss:.4f}, ValLoss: {val_loss:.4f}, TrainAcc: {train_acc:.2f},% ValAcc: {val_acc:.2f}%\")\n","\n","      def evaluate(self, val_dl, pad_idx = 0):\n","        losses = []\n","        accuracies = []\n","        with torch.no_grad():\n","          for batch in val_dl:\n","              text = batch[0]\n","              output = batch[1]\n","              output = output.transpose(0,1)\n","              tags = output[0]\n","              mask = output[1]\n","              predictions = self.forward(text)\n","              predictions = predictions.transpose(1,2)\n","              loss = self.loss_fn(predictions, tags)\n","              loss = (loss * mask).sum() / mask.sum()\n","              losses.append(loss)\n","              acc = self.categorical_accuracy(predictions, tags, mask, pad_idx)\n","              accuracies.append(acc)\n","        return torch.Tensor(losses).mean(), torch.Tensor(accuracies).mean()\n","\n","\n","    class OHGTagger(Model):\n","        def __init__(self, vocab, embeds, hidden, tagset, n_layers, dropout, criterion, pad_idx = 0):\n","            super().__init__()\n","\n","            self.embedding = nn.Embedding(vocab, embeds, padding_idx = pad_idx, scale_grad_by_freq = True)\n","\n","            self.lstm = nn.LSTM(embeds, hidden, num_layers = n_layers, bidirectional = True,\n","                                dropout = dropout if n_layers > 1 else 0)\n","\n","            self.lin = nn.Linear(hidden * 2, tagset)\n","\n","            self.dropout = nn.Dropout(dropout)\n","\n","            self.loss_fn = criterion\n","\n","        def forward(self, sent):\n","            x = self.embedding(sent)\n","            x, (hidden, cell) = self.lstm(x)\n","            x = self.lin(self.dropout(x))\n","            return x\n","\n","    vocab = len(allwords)+1\n","    n_emb = 300\n","    n_hidden = 60\n","    n_tags = len(alltags)+1\n","    n_layers = 3\n","    dropout = .2\n","    pad_idx = 0\n","    criterion = nn.CrossEntropyLoss(reduction = 'none', weight = class_weights, ignore_index = pad_idx).to(device)\n","\n","    network_OHGTagger = OHGTagger(vocab, n_emb, n_hidden, n_tags, n_layers, dropout, criterion, pad_idx).to(device)\n","\n","    %time network_OHGTagger.fit(train_loader, test_loader, 100)\n","\n","    preds, tags, texts = predict(network_OHGTagger, test_loader, pad_idx)\n","\n","    for i in range(len(preds)):\n","      for j in range(len(preds[i])):\n","        if tags[i][j] == 0:\n","          preds[i][j] = 0\n","      preds[i] = [ele for ele in preds[i] if ele != 0]\n","      tags[i] = [ele for ele in tags[i] if ele != 0]\n","      texts[i] = [ele for ele in texts[i] if ele != 0]\n","    decpreds, dectags, dectext, declangs = [], [], [], []\n","    decpreds.append(tokenize(preds, tag_decoder))\n","    dectags.append(tokenize(tags, tag_decoder))\n","    dectext.append(tokenize(texts, word_decoder))\n","    decpreds = decpreds[0]\n","    dectags = dectags[0]\n","    dectext = dectext[0]\n","\n","    in_set_vocab = []\n","    for batch in train_loader:\n","      texts = batch[0]\n","      for sent in texts:\n","        sent = sent.tolist()\n","        sent = [ele for ele in sent if ele != 0]\n","        x = []\n","        for i in sent:\n","          x.append([i])\n","        x = tokenize(x, word_decoder)\n","        for word in x:\n","          in_set_vocab.append(word)\n","    in_set_vocab = set([x for y in in_set_vocab for x in y])\n","\n","    corrects, incorrects, totals = [], [], []\n","    uk_corrects, uk_incorrects, uk_totals = [], [], []\n","    for i in range(len(decpreds)):\n","      for j in range(len(decpreds[i])):\n","        totals.append(dectags[i][j])\n","        if decpreds[i][j] == dectags[i][j]:\n","          corrects.append(decpreds[i][j])\n","        else:\n","          incorrects.append(decpreds[i][j])\n","        if dectext[i][j] not in in_set_vocab:\n","          uk_totals.append(dectags[i][j])\n","          if decpreds[i][j] == dectags[i][j]:\n","            uk_corrects.append(decpreds[i][j])\n","          else:\n","            uk_incorrects.append(decpreds[i][j])\n","\n","    c_freqs = Counter(corrects)\n","    ic_freqs = Counter(incorrects)\n","    t_freqs = Counter(totals)\n","    uk_c_freqs = Counter(uk_corrects)\n","    uk_ic_freqs = Counter(uk_incorrects)\n","    uk_t_freqs = Counter(uk_totals)\n","\n","    data = []\n","    labels = []\n","    outs = []\n","    any_c = [x[0] for x in c_freqs.most_common()]\n","    for i in t_freqs.most_common():\n","      if i[0] not in any_c:\n","        data.append(0)\n","        labels.append(i[0])\n","        outs.append((i[0], 0))\n","        results_df.loc[len(results_df)] = [i[0], 0, size]\n","      else:\n","        for ele in c_freqs.most_common():\n","          if ele[0] == i[0]:\n","            num_c = ele[1]\n","        class_acc = num_c/i[1]\n","        data.append(class_acc*100)\n","        labels.append(i[0])\n","        outs.append((i[0], class_acc*100))\n","        results_df.loc[len(results_df)] = [i[0], class_acc*100, size]\n","\n","  results_df.to_csv('resultsOHG_test.csv', index=False)\n","  with torch.no_grad():\n","      torch.cuda.empty_cache()\n","  iteration = iteration + 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rwd9TQ0AfLjg"},"outputs":[],"source":["results_df.to_csv('resultsOHG_tes.csv', index=False)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.8"}},"nbformat":4,"nbformat_minor":0}